# Local LLM Configuration (OpenAI-compatible API)
# For Ollama (default): http://localhost:11434/v1
# For LM Studio: http://localhost:1234/v1
# For LocalAI: http://localhost:8080/v1
LLM_BASE_URL=http://localhost:11434/v1

# Model name (depends on your LLM provider)
# Ollama: llama3.1, codellama, mistral, etc.
# LM Studio: Whatever model you loaded
LLM_MODEL=llama3.1

# API key (Ollama doesn't need a real key, just use 'ollama')
LLM_API_KEY=ollama

# Supabase Configuration
# Get these from your Supabase project settings
SUPABASE_URL=https://your-project-id.supabase.co
SUPABASE_ANON_KEY=your-anon-key-here

# Optional: Custom inklecate binary path
# Leave blank to auto-detect
INKLECATE_PATH=
